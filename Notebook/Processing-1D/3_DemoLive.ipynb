{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcUDwPYKQtUd"
   },
   "source": [
    "# Progetto DSIM - Demo Live Audio\n",
    "\n",
    "---\n",
    "\n",
    "Autori: Confalonieri Riccardo | Mariani Ginevra | Mora Lorenzo <br>\n",
    "E-mail: r.confalonieri5@campus.unimib.it | g.mariani34@campus.unimib.it |  l.mora4@campus.unimib.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyEy7nPtQtUk"
   },
   "source": [
    "Lo scopo di questo notebook è implementare una live demo per dimostrare l'efficacia dei modelli implementati per il task di speech recognition. Siccome la demo necessita del microfono (e della telecamera anche se non essenziale) per acquisire gli audio è necessario eseguire il file localmente e non in Colab! Inoltre è importante controllare il path da cui si caricano i modelli trainati per evitare errori. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n8sm7dsQtUo"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlfUL7TcOlLi"
   },
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from playsound import playsound\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import load_model\n",
    "import cv2 as cv\n",
    "import pickle\n",
    "from scipy.io import wavfile as wav\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGXe6rxyQtUv"
   },
   "source": [
    "## Extract features\n",
    "In questa sezione vengono riportati tutti i metodi necessari per estrarre le features di riferimento in base al modello utilizzato per la predizione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqAqfVT1qvOH"
   },
   "outputs": [],
   "source": [
    "def check_length(input, size):\n",
    "    # Elimina eventuali valori oltre il numero consentito\n",
    "    output = input[0:min(size, input.shape[0])]\n",
    "    # Aggiungi valori nulli per raggiungere la dimensione richiesta\n",
    "    output = np.concatenate((output, np.zeros(size-output.shape[0])))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIWs7UgoxiVi"
   },
   "outputs": [],
   "source": [
    "# ritorna mfcc dell'audio\n",
    "def mfcc(input, rate = 44100):\n",
    "    return librosa.feature.mfcc(input, sr = rate, n_mfcc = 13)\n",
    "\n",
    "# crea lo spettrogramma di un audio \n",
    "def get_spectrogram(path, output_shape = (64, 64)):\n",
    "    data, samplerate = sf.read(path)\n",
    "    # data = check_length(data, multi = True)[2]\n",
    "    # Spettrogramma\n",
    "    audio_stft = librosa.amplitude_to_db(np.abs(librosa.stft(data.astype(float))))\n",
    "    scaled_stft = audio_stft + abs(np.min(audio_stft))\n",
    "\n",
    "    image = scaled_stft/np.max(scaled_stft)*255\n",
    "    # Espansione dell'immagine a tre dimensioni\n",
    "    image = np.repeat(np.expand_dims(image, 2), 3, 2).astype('uint8')\n",
    "    # Resizing\n",
    "    image = cv.resize(image, output_shape)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-r2V19UQtUy"
   },
   "source": [
    "## Demo\n",
    "Questa sezione riporta le funzioni necessarie per implementare la demo correttamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20o2zGh91Ulr"
   },
   "outputs": [],
   "source": [
    "def predictVoice(audio_path, size, model):\n",
    "    '''\n",
    "      Determina lo speaker dell'audio in input.\n",
    "\n",
    "      :param audio_path = percorso per audio da classificare\n",
    "      :param size = dimensione default dell'audio in secondi\n",
    "      :param model = modello per la classificazione\n",
    "    '''\n",
    "\n",
    "    label_authors = {0: 'Ginevra', 1:'Lorenzo', 2:'Riccardo'}\n",
    "      \n",
    "    author = -1 # nel caso di errore\n",
    "    sr, audio = wav.read(audio_path)\n",
    "    # verifico che l'audio sia della lunghezza giusta, nel caso lo correggo\n",
    "    audio = check_length(audio, size*sr)\n",
    "        \n",
    "    \n",
    "    if isinstance(model, sklearn.model_selection._search.GridSearchCV):\n",
    "        features = mfcc(audio).flatten()\n",
    "        author = model.predict(features.reshape(-1, features.shape[0]))[0]\n",
    "        \n",
    "    \n",
    "    elif (isinstance(model, keras.Model) and \n",
    "        list(model.input.shape) == [None, 64, 64, 3]):\n",
    "        img = get_spectrogram(audio_path)\n",
    "        image = np.expand_dims(img, axis = 0)\n",
    "\n",
    "        author = model.predict(image)\n",
    "        # estraggo il singolo autore\n",
    "        author = np.argmax(author, axis = 1)\n",
    "        # converto da numero a stringa di riferimento\n",
    "        author = label_authors[author[0]]\n",
    "        \n",
    "    elif isinstance(model, keras.Model):\n",
    "        # leggo l'audio dal file\n",
    "        _, audio = wav.read(audio_path)\n",
    "            \n",
    "        # estraggo le features con mfcc\n",
    "        feats = mfcc(audio)\n",
    "        \n",
    "        # adatto le features alla shape richiesta dal modello\n",
    "        feats = np.array(feats).reshape(-1,feats.shape[0],feats.shape[1],1) \n",
    "        # effettuo la predizione\n",
    "        author = model.predict(feats)\n",
    "        # estraggo il singolo autore\n",
    "        author = np.argmax(author, axis = 1)\n",
    "        # converto da numero a stringa di riferimento\n",
    "        author = label_authors[author[0]]\n",
    "        \n",
    "        \n",
    "    return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_L3LwCOz751K"
   },
   "outputs": [],
   "source": [
    "# funzione che salva il file su disco\n",
    "def save_file_helper(audio, rec_rate):\n",
    "    # Salvataggio registrazione voce\n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%m-%d-%Y_%H%M%S', t)\n",
    "    nome_file = 'rec_' + timestamp + '.wav'\n",
    "    write(nome_file, rate = rec_rate, data = audio)\n",
    "    \n",
    "    return nome_file\n",
    "\n",
    "\n",
    "def demo(model, duration = 5, rec_rate = 44100, archive = False):\n",
    "    '''\n",
    "        :param model = classificatore per riconoscimento vocale\n",
    "        :param duration = durata dei file audio in secondi\n",
    "        :param rec_rate = sample rate dei file audio\n",
    "        :param archive = True per memorizzare file\n",
    "        \n",
    "    '''\n",
    "    comando = \"Premere 'spacebar' per registrare o 'q' per fermare la registrazione.\"\n",
    "    registra = False\n",
    "    wait_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        videoCap = cv.VideoCapture(0)\n",
    "\n",
    "        if (videoCap.isOpened() == False):\n",
    "            print(\"Errore apertura video camera o file.\")\n",
    "\n",
    "        while(videoCap.isOpened()):\n",
    "            frameOk, frame = videoCap.read()\n",
    "\n",
    "            if not frameOk:\n",
    "                print('Impossibile riconoscere l\\'immagine')\n",
    "                # Break the loop\n",
    "                break\n",
    "\n",
    "            # se l'utente ha cliccato la barra e sono passati 0.3 secondi inizio a registrare\n",
    "            # gli 0.3 sec non sono necessari sono solo un aiuto per iniziare a parlare e registrare\n",
    "            # un audio di solo parole effettive\n",
    "            if registra and (time.time() - wait_time > 0.3):\n",
    "                # registro l'audio della lunghezza necessaria\n",
    "                audio = sd.rec(int(duration * rec_rate), \n",
    "                               samplerate = rec_rate, channels = 1, blocking=True)\n",
    "\n",
    "\n",
    "                # salvo l'audio su disco per processarlo correttamente\n",
    "                tmp_filename = save_file_helper(audio, rec_rate)\n",
    "\n",
    "                # Identificazione speaker attraverso modello\n",
    "                author = predictVoice(tmp_filename, 2, model)\n",
    "\n",
    "                # aggiorno il comando\n",
    "                if author == 'Ginevra':\n",
    "                    comando = \"Beccata \" + author + \"! Come stai?\"\n",
    "                else:\n",
    "                    comando = \"Beccato \" + author + \"! Come stai?\"\n",
    "\n",
    "\n",
    "                # risetto a false la variabile registra\n",
    "                registra = False\n",
    "\n",
    "                # se non richiesto cancello il file da disco\n",
    "                if archive == False:\n",
    "                    os.remove(tmp_filename)\n",
    "\n",
    "                wait_time = time.time()\n",
    "\n",
    "            # verifico se ha premuto lo spazio, nel caso cambio comando da visualizzare\n",
    "            # e mi preparo per registrare l'audio\n",
    "            if cv.waitKey(25) & 0xFF == ord(' '):\n",
    "                comando = 'Sto registrando l\\'audio...'\n",
    "                wait_time = time.time()\n",
    "                registra = True\n",
    "\n",
    "\n",
    "            # Premere 'q' per uscire    \n",
    "            if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            # risetto il comando iniziale dopo che ho predetto l'utente\n",
    "            if (registra == False) and (time.time() - wait_time > 2):\n",
    "                comando = \"Premere 'spacebar' per registrare o 'q' per fermare la registrazione.\"\n",
    "\n",
    "            # cambio comando e visualizzazione su video\n",
    "            cv.putText(frame, comando, (15, 37), \n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.75, (20,38,65), 2)\n",
    "            # visualizzo la finestra\n",
    "            cv.imshow('Riconoscimento identita\\' dello speaker.', frame)\n",
    "\n",
    "\n",
    "        # Rilascia la video camera quando tutto è eseguito\n",
    "        videoCap.release()\n",
    "        cv.destroyAllWindows()\n",
    "    except:\n",
    "        # Rilascia la video camera quando tutto è eseguito\n",
    "        videoCap.release()\n",
    "        cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-IwG4UgQtU3"
   },
   "outputs": [],
   "source": [
    "model = load_model('./scratch.h5')\n",
    "# model = load_model('./modelSpectrogram_compressed.h5')\n",
    "# model = pickle.load(open('./svm_augm.pkl', 'rb'))\n",
    "demo(model, 2, 44100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3887,
     "status": "ok",
     "timestamp": 1644440236951,
     "user": {
      "displayName": "Riccardo Confalonieri",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05953771849724216239"
     },
     "user_tz": -60
    },
    "id": "7M42LlwnRfHl",
    "outputId": "5d98963d-580a-489b-e62d-7b3816faa278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /content/drive/MyDrive/Confalonieri_Mariani_Mora_DSIM/Notebook/Processing-1D/3_DemoLive.ipynb to html\n",
      "[NbConvertApp] Writing 305701 bytes to /content/drive/MyDrive/Confalonieri_Mariani_Mora_DSIM/Notebook/Processing-1D/3_DemoLive.html\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download notebook in .html extension\n",
    "%%shell\n",
    "jupyter nbconvert --to html '/content/drive/MyDrive/Confalonieri_Mariani_Mora_DSIM/Notebook/Processing-1D/3_DemoLive.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liUp20EMRqMC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_DemoLive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
